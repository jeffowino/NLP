{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WHATSAPP SENTIMENT ANALYSIS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffowino/NLP/blob/main/WHATSAPP_SENTIMENT_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKYnsTNTtzbJ"
      },
      "source": [
        "To analyze the sentiments of a WhatsApp chat, we need to collect data from WhatsApp. Most of you must be using this messaging app, so to collect data about your chat, simply follow the steps mentioned below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__G-RWvdt8Wn"
      },
      "source": [
        "For iPhone:\n",
        "Open your chat with a person or a group\n",
        "Just tap on the profile of the person or the group\n",
        "You will see an option to export chat down below\n",
        "For Android:\n",
        "Open your chat with a person or a group \n",
        "Click on the three dots above \n",
        "Click on more\n",
        "Click on the export chat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UEkV_fmasoXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qSzQgRTRsowe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQmoENqAuBN_"
      },
      "source": [
        "You will see an option to attach media while exporting your chat. For simplicity, it is best not to attach media. Finally, enter your email and you will find your WhatsApp chat in your inbox."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmB8ErFIuFeL"
      },
      "source": [
        "WhatsApp Chat Sentiment Analysis using Python\n",
        "Now let’s start with the task of WhatsApp chat sentiment analysis with Python. I’ll start this task by defining some helper functions because the data we get from WhatsApp is not a dataset that is ready to be used for any kind of data science task. So, to prepare your data for the sentiment analysis task, just define all the functions as defined below: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqpAUsxqtu7p"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmafmyd4tt07",
        "outputId": "7deea8fc-235e-4cc3-f0bb-7bac01658e45"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install emoji\n",
        "import emoji\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "# Extract Time\n",
        "def date_time(s):\n",
        "    pattern = '^([0-9]+)(\\/)([0-9]+)(\\/)([0-9]+), ([0-9]+):([0-9]+)[ ]?(AM|PM|am|pm)? -'\n",
        "    result = re.match(pattern, s)\n",
        "    if result:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Find Authors or Contacts\n",
        "def find_author(s):\n",
        "    s = s.split(\":\")\n",
        "    if len(s)==2:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Finding Messages\n",
        "def getDatapoint(line):\n",
        "    splitline = line.split(' - ')\n",
        "    dateTime = splitline[0]\n",
        "    date, time = dateTime.split(\", \")\n",
        "    message = \" \".join(splitline[1:])\n",
        "    if find_author(message):\n",
        "        splitmessage = message.split(\": \")\n",
        "        author = splitmessage[0]\n",
        "        message = \" \".join(splitmessage[1:])\n",
        "    else:\n",
        "        author= None\n",
        "    return date, time, author, message"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 4.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=b69c0c8ebacb46693856523b708596a710619c135b1c7448249e7a17cb3286ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QGs2quuuO1Z"
      },
      "source": [
        "It doesn’t matter if you are using a group chat dataset or your conversation with one person. All the functions defined above will prepare your data for the task of sentiment analysis as well as for any data science task. Now here is how we can prepare the data we collected from WhatsApp by using the above functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cskb8J0DsW3u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mobjeKVdrxCn",
        "outputId": "316008f4-5c8a-4927-ecf7-285306264afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "data = []\n",
        "conversation = '/content/_chat.txt'\n",
        "with open(conversation, encoding=\"utf-8\") as fp:\n",
        "    fp.readline()\n",
        "    messageBuffer = []\n",
        "    date, time, author = None, None, None\n",
        "    while True:\n",
        "        line = fp.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        line = line.strip()\n",
        "        if date_time(line):\n",
        "            if len(messageBuffer) > 0:\n",
        "                data.append([date, time, author, ' '.join(messageBuffer)])\n",
        "            messageBuffer.clear()\n",
        "            date, time, author, message = getDatapoint(line)\n",
        "            messageBuffer.append(message)\n",
        "        else:\n",
        "            messageBuffer.append(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a88ef0c260ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconversation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/_chat.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmessageBuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/_chat.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Klao8r4EtCzJ",
        "outputId": "da94c2ad-0858-447d-c8ba-e8293344cb8d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLyHXCEJsnOx"
      },
      "source": [
        "Now here is how we can analyze the sentiments of WhatsApp chat using Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-A6n5jfsgia",
        "outputId": "4c98505b-2e74-4ce1-de1a-e6a605ec9ed7"
      },
      "source": [
        "df = pd.DataFrame(data, columns=[\"Date\", 'Time', 'Author', 'Message'])\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "data = df.dropna()\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sentiments = SentimentIntensityAnalyzer()\n",
        "data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"Message\"]]\n",
        "data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"Message\"]]\n",
        "data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"Message\"]]\n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Date, Time, Author, Message, Positive, Negative, Neutral]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyvlYeOrtImp",
        "outputId": "f99bded6-d350-42d6-97e6-46edc32d9c99"
      },
      "source": [
        "x = sum(data[\"Positive\"])\n",
        "y = sum(data[\"Negative\"])\n",
        "z = sum(data[\"Neutral\"])\n",
        "\n",
        "def sentiment_score(a, b, c):\n",
        "    if (a>b) and (a>c):\n",
        "        print(\"Positive 😊 \")\n",
        "    elif (b>a) and (b>c):\n",
        "        print(\"Negative 😠 \")\n",
        "    else:\n",
        "        print(\"Neutral 🙂 \")\n",
        "sentiment_score(x, y, z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neutral 🙂 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk1UfESytnzQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzhWP5jJtX-R"
      },
      "source": [
        "So, the data I used indicates that most of the messages between me and the other person are neutral. Which means it’s neither positive nor negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQnMufoNtdsz"
      },
      "source": [
        "Summary\n",
        "So this is how we can perform the task of sentiment analysis of WhatsApp chat. WhatsApp is a great source of data for the task of sentiment analysis and every data science task based on natural language processing. I hope you liked this article on the task of WhatsApp chat sentiment analysis using Python. Feel free to ask your valuable questions in the comments section below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQPSy6XRuuwz"
      },
      "source": [
        "NOW LETS US DO GENERAL WHATSAPP ANALYSIS OF CHATS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEK1GDAOuzEt"
      },
      "source": [
        "I hope you now have understood how to get your WhatsApp data for the task of WhatsApp chat analysis with Python. Now let’s start this task by importing the necessary Python libraries that we need for this task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCik42auu3YZ"
      },
      "source": [
        "import regex\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8XJV01gu7Ob"
      },
      "source": [
        "The dataset we are using here requires a lot of preparation, so I suggest you take a look at the data you are using before starting this WhatsApp chat analysis task. As I have already walked through the dataset, so I’ll start by writing a few Python functions to prepare the data before importing it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL08HsTVu8Ee"
      },
      "source": [
        "def date_time(s):\n",
        "    pattern = '^([0-9]+)(\\/)([0-9]+)(\\/)([0-9]+), ([0-9]+):([0-9]+)[ ]?(AM|PM|am|pm)? -'\n",
        "    result = regex.match(pattern, s)\n",
        "    if result:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def find_author(s):\n",
        "    s = s.split(\":\")\n",
        "    if len(s)==2:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def getDatapoint(line):\n",
        "    splitline = line.split(' - ')\n",
        "    dateTime = splitline[0]\n",
        "    date, time = dateTime.split(\", \")\n",
        "    message = \" \".join(splitline[1:])\n",
        "    if find_author(message):\n",
        "        splitmessage = message.split(\": \")\n",
        "        author = splitmessage[0]\n",
        "        message = \" \".join(splitmessage[1:])\n",
        "    else:\n",
        "        author= None\n",
        "    return date, time, author, message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIW6OQ2wvE9b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilFJE_g_vHwH"
      },
      "source": [
        "Now let’s import the data and prepare it in a way that we can use it in a pandas DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVY3zEkPvIj_"
      },
      "source": [
        "data = []\n",
        "conversation = '/content/_chat.txt'\n",
        "with open(conversation, encoding=\"utf-8\") as fp:\n",
        "    fp.readline()\n",
        "    messageBuffer = []\n",
        "    date, time, author = None, None, None\n",
        "    while True:\n",
        "        line = fp.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        line = line.strip()\n",
        "        if date_time(line):\n",
        "            if len(messageBuffer) > 0:\n",
        "                data.append([date, time, author, ' '.join(messageBuffer)])\n",
        "            messageBuffer.clear()\n",
        "            date, time, author, message = getDatapoint(line)\n",
        "            messageBuffer.append(message)\n",
        "        else:\n",
        "            messageBuffer.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ugATK7S9yG7V",
        "outputId": "bb7f4d95-8957-4b74-a103-89ea383cc1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/_chat.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHltOgWXvXkL"
      },
      "source": [
        "Our dataset is completely ready now for the task of WhatsApp chat analysis with Python. Now let’s have a look at the last 20 messages and some other insights from the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu-seElGvUDv",
        "outputId": "f5969418-d363-4d58-b85d-168cab59789b"
      },
      "source": [
        "df = pd.DataFrame(data, columns=[\"Date\", 'Time', 'Author', 'Message'])\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "print(df.tail(20))\n",
        "print(df.info())\n",
        "print(df.Author.unique())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Date, Time, Author, Message]\n",
            "Index: []\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 0 entries\n",
            "Data columns (total 4 columns):\n",
            " #   Column   Non-Null Count  Dtype         \n",
            "---  ------   --------------  -----         \n",
            " 0   Date     0 non-null      datetime64[ns]\n",
            " 1   Time     0 non-null      object        \n",
            " 2   Author   0 non-null      object        \n",
            " 3   Message  0 non-null      object        \n",
            "dtypes: datetime64[ns](1), object(3)\n",
            "memory usage: 0.0+ bytes\n",
            "None\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFult1QuvoD5"
      },
      "source": [
        "Now let’s have a look at the total number of messages between this WhatsApp chat:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB-obsFbve5P",
        "outputId": "2a551e95-e896-410d-ab7d-23206d73d854"
      },
      "source": [
        "total_messages = df.shape[1]\n",
        "print(total_messages)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HvLi1htvwx0"
      },
      "source": [
        "Now let’s have a look at the total number of media messages present in this chat:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAyIYHmGvxeW",
        "outputId": "32171066-ebbe-40f5-a2d8-b9af169e577d"
      },
      "source": [
        "media_messages = df[df[\"Message\"]=='<Media omitted>'].shape[0]\n",
        "print(media_messages)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CEcJgpsv5sb"
      },
      "source": [
        "Now let’s extract the emojis present in between the chats and have a look at the emojis present in this chat:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhGuvO4fv05z",
        "outputId": "935dcd33-7014-4e95-9beb-fb89f5781555"
      },
      "source": [
        "def split_count(text):\n",
        "    emoji_list = []\n",
        "    data = regex.findall(r'\\X',text)\n",
        "    for word in data:\n",
        "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
        "            emoji_list.append(word)\n",
        "    return emoji_list\n",
        "df['emoji'] = df[\"Message\"].apply(split_count)\n",
        "\n",
        "emojis = sum(df['emoji'].str.len())\n",
        "print(emojis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC_LRn24wGkj"
      },
      "source": [
        "Now let’s extract the URLs present in this chat and have a look at the final insights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M14ZZtswFFk",
        "outputId": "1566f20d-405b-4b28-d1e7-ae351c36334d"
      },
      "source": [
        "URLPATTERN = r'(https?://\\S+)'\n",
        "df['urlcount'] = df.Message.apply(lambda x: regex.findall(URLPATTERN, x)).str.len()\n",
        "links = np.sum(df.urlcount)\n",
        "\n",
        "print(\"Chats between Aman and Sapna\")\n",
        "print(\"Total Messages: \", total_messages)\n",
        "print(\"Number of Media Shared: \", media_messages)\n",
        "print(\"Number of Emojis Shared\", emojis)\n",
        "print(\"Number of Links Shared\", links)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chats between Aman and Sapna\n",
            "Total Messages:  0\n",
            "Number of Media Shared:  0\n",
            "Number of Emojis Shared 0\n",
            "Number of Links Shared 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1TESP4SwYNS"
      },
      "source": [
        "Now let’s prepare this data to get more insights to analyze all the messages sent in this chat in more detail:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "7SCfC3KGwPpR",
        "outputId": "21584d45-b165-4336-92a0-d83afc64acf9"
      },
      "source": [
        "media_messages_df = df[df['Message'] == '<Media omitted>']\n",
        "messages_df = df.drop(media_messages_df.index)\n",
        "messages_df['Letter_Count'] = messages_df['Message'].apply(lambda s : len(s))\n",
        "messages_df['Word_Count'] = messages_df['Message'].apply(lambda s : len(s.split(' ')))\n",
        "messages_df[\"MessageCount\"]=1\n",
        "\n",
        "l = [\"Hadi Abdullahi\",\"Ahmey Yussuf\"]\n",
        "for i in range(len(l)):\n",
        "  # Filtering out messages of particular user\n",
        "  req_df= messages_df[messages_df[\"Author\"] == l[i]]\n",
        "  # req_df will contain messages of only one particular user\n",
        "  print(f'Stats of {l[i]} -')\n",
        "  # shape will print number of rows which indirectly means the number of messages\n",
        "  print('Messages Sent', req_df.shape[0])\n",
        "  #Word_Count contains of total words in one message. Sum of all words/ Total Messages will yield words per message\n",
        "  words_per_message = (np.sum(req_df['Word_Count']))/req_df.shape[0]\n",
        "  print('Average Words per message', words_per_message)\n",
        "  #media conists of media messages\n",
        "  media = media_messages_df[media_messages_df['Author'] == l[i]].shape[0]\n",
        "  print('Media Messages Sent', media)\n",
        "  # emojis conists of total emojis\n",
        "  emojis = sum(req_df['emoji'].str.len())\n",
        "  print('Emojis Sent', emojis)\n",
        "  #links consist of total links\n",
        "  links = sum(req_df[\"urlcount\"])   \n",
        "  print('Links Sent', links)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats of Hadi Abdullahi -\n",
            "Messages Sent 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-45178731a735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Messages Sent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m#Word_Count contains of total words in one message. Sum of all words/ Total Messages will yield words per message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mwords_per_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Word_Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mreq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average Words per message'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_per_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m#media conists of media messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzpIDifLwiNo"
      },
      "source": [
        "Now let’s prepare a visualization of the total emojis present in the chat and the type of emojis sent between the two people. It will help in understanding the relationship between both the people:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB_ZEzHZwm1U"
      },
      "source": [
        "total_emojis_list = list(set([a for b in messages_df.emoji for a in b]))\n",
        "total_emojis = len(total_emojis_list)\n",
        "\n",
        "total_emojis_list = list([a for b in messages_df.emoji for a in b])\n",
        "emoji_dict = dict(Counter(total_emojis_list))\n",
        "emoji_dict = sorted(emoji_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "for i in emoji_dict:\n",
        "  print(i)\n",
        "  \n",
        "emoji_df = pd.DataFrame(emoji_dict, columns=['emoji', 'count'])\n",
        "import plotly.express as px\n",
        "fig = px.pie(emoji_df, values='count', names='emoji')\n",
        "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTzcjmmTwwsH"
      },
      "source": [
        "Now let’s have a look at the most used words in this WhatsApp chat by visualizing a word cloud:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z87FT2zwx-i"
      },
      "source": [
        "text = \" \".join(review for review in messages_df.Message)\n",
        "print (\"There are {} words in all the messages.\".format(len(text)))\n",
        "stopwords = set(STOPWORDS)\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.figure( figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka2C6sFSw5WD"
      },
      "source": [
        "Now let’s have a look at the most used words by each person by visualizing two different word clouds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wxrQI-ow9PT"
      },
      "source": [
        "l = [\"Dr. Lawrence Nderu\",\"Dan Jenga\", \"Jenga Alfred\",\"Dr kenagi\",\"Mahehu Jenga\",\"Jeff\"]\n",
        "for i in range(len(l)):\n",
        "  dummy_df = messages_df[messages_df['Author'] == l[i]]\n",
        "  text = \" \".join(review for review in dummy_df.Message)\n",
        "  stopwords = set(STOPWORDS)\n",
        "  #Generate a word cloud image\n",
        "  print('Author name',l[i])\n",
        "  wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
        "  #Display the generated image   \n",
        "  plt.figure( figsize=(10,5))\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVflsI7WxrN5"
      },
      "source": [
        "Summary\n",
        "So this is how we can easily analyze any WhatsApp chat between you and your friend, customer, or even a group of people. You can further use this data for many other tasks of natural language processing. I hope you liked this article on the task of WhatsApp chat analysis with Python. Feel free to ask your valuable questions in the comments section below."
      ]
    }
  ]
}